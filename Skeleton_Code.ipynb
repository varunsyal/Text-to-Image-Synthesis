{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pickle\n",
    "from random import *\n",
    "import gc\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import torch.optim as optim\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_desc_per_img = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data on System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(dataset='CUB', process='resize'):\n",
    "    # Define File Destinations\n",
    "    test_desc_fname = dataset+'/desc/test/char-CNN-RNN-embeddings.npy'\n",
    "    train_desc_fname = dataset+'/desc/train/char-CNN-RNN-embeddings.npy'\n",
    "    \n",
    "    test_files_fname = dataset+'/desc/test/filenames.pickle'\n",
    "    train_files_fname = dataset+'/desc/train/filenames.pickle'\n",
    "    text_dir = dataset+'/desc/text_c10/'\n",
    "    \n",
    "    test_img_dir = dataset+'/images/'\n",
    "    train_img_dir = dataset+'/images/'\n",
    "    \n",
    "    train_s1_imgs = []\n",
    "    test_s1_imgs = []\n",
    "    train_s2_imgs = []\n",
    "    test_s2_imgs = []\n",
    "    train_s1_emds = []\n",
    "    test_s1_emds = []\n",
    "    train_s2_emds = []\n",
    "    test_s2_emds = []\n",
    "    train_text = []\n",
    "    test_text = []\n",
    "    \n",
    "    #Load Training Data\n",
    "    print('Loading Training Data...')\n",
    "    train_embed = np.load(train_desc_fname)\n",
    "    train_embed_shape = train_embed.shape\n",
    "    \n",
    "    with open(train_files_fname,'rb') as file:\n",
    "        dat = pickle.load(file)\n",
    "    for i in range(len(dat)):\n",
    "        for l in open(text_dir+dat[i]+'.txt','rb'):\n",
    "            train_text.append(l.strip().decode('utf-8'))\n",
    "        \n",
    "        img = cv2.imread(train_img_dir+dat[i]+'.jpg',1)\n",
    "        if process == 'resize':\n",
    "            img_s1 = cv2.resize(img,(64,64),interpolation=cv2.INTER_AREA)\n",
    "            img_s2 = cv2.resize(img,(256,256),interpolation=cv2.INTER_AREA)\n",
    "        else:\n",
    "            pass\n",
    "        img_s1 = np.transpose(img_s1,(2,0,1))\n",
    "        img_s2 = np.transpose(img_s2,(2,0,1))\n",
    "        train_s1_imgs.append(img_s1)\n",
    "        train_s2_imgs.append(img_s2)\n",
    "        for j in range(train_embed_shape[1]):\n",
    "#             neg_img = randint(0,train_embed_shape[0]-1)\n",
    "#             while neg_img == i:\n",
    "#                 neg_img = randint(0,train_embed_shape[0]-1)\n",
    "#             neg_idx = randint(0,train_embed_shape[1]-1)\n",
    "#             train_s1_imgs.append(img_s1)\n",
    "            train_s1_emds.append(train_embed[i,j,:])\n",
    "#             train_s1_imgs.append(img_s1)\n",
    "#             train_s1_emds.append(train_embed[neg_img,neg_idx,:])\n",
    "#             train_s2_imgs.append(img_s2)\n",
    "            train_s2_emds.append(train_embed[i,j,:])\n",
    "#             train_s2_imgs.append(img_s2)\n",
    "#             train_s2_emds.append(train_embed[neg_img,neg_idx,:])\n",
    "#             train_s1_data.append((img_s1,train_embed[i,j,:],1))\n",
    "#             train_s1_data.append((img_s1,train_embed[neg_img,neg_idx,:],0))\n",
    "#             train_s2_data.append((img_s2,train_embed[i,j,:],1))\n",
    "#             train_s2_data.append((img_s2,train_embed[neg_img,neg_idx,:],0))\n",
    "        \n",
    "        if i%1000 == 0:\n",
    "            print(i)\n",
    "        \n",
    "    #Load Testing Data\n",
    "    print('Loading Testing Data...')\n",
    "    test_embed = np.load(test_desc_fname)\n",
    "    test_embed_shape = test_embed.shape\n",
    "    \n",
    "    with open(test_files_fname,'rb') as file:\n",
    "        dat = pickle.load(file)\n",
    "    for i in range(len(dat)):\n",
    "        for l in open(text_dir+dat[i]+'.txt','rb'):\n",
    "            test_text.append(l.strip().decode('utf-8'))\n",
    "        \n",
    "        img = cv2.imread(test_img_dir+dat[i]+'.jpg',1)\n",
    "        if process == 'resize':\n",
    "            img_s1 = cv2.resize(img,(64,64),interpolation=cv2.INTER_AREA)\n",
    "            img_s2 = cv2.resize(img,(256,256),interpolation=cv2.INTER_AREA)\n",
    "        else:\n",
    "            pass\n",
    "        img_s1 = np.transpose(img_s1,(2,0,1))\n",
    "        img_s2 = np.transpose(img_s2,(2,0,1))\n",
    "        test_s1_imgs.append(img_s1)\n",
    "        test_s2_imgs.append(img_s2)\n",
    "        for j in range(test_embed_shape[1]):\n",
    "#             neg_img = randint(0,test_embed_shape[0]-1)\n",
    "#             while neg_img == i:\n",
    "#                 neg_img = randint(0,test_embed_shape[0]-1)\n",
    "#             neg_idx = randint(0,test_embed_shape[1]-1)\n",
    "#             test_s1_imgs.append(img_s1)\n",
    "            test_s1_emds.append(train_embed[i,j,:])\n",
    "#             test_s1_imgs.append(img_s1)\n",
    "#             test_s1_emds.append(train_embed[neg_img,neg_idx,:])\n",
    "#             test_s2_imgs.append(img_s2)\n",
    "            test_s2_emds.append(train_embed[i,j,:])\n",
    "#             test_s2_imgs.append(img_s2)\n",
    "#             test_s2_emds.append(train_embed[neg_img,neg_idx,:])\n",
    "#             test_s1_data.append((img_s1,test_embed[i,j,:],1))\n",
    "#             test_s1_data.append((img_s1,test_embed[neg_img,neg_idx,:],0))\n",
    "#             test_s2_data.append((img_s2,test_embed[i,j,:],1))\n",
    "#             test_s2_data.append((img_s2,test_embed[neg_img,neg_idx,:],0))\n",
    "            \n",
    "        if i%1000 == 0:\n",
    "            print(i)\n",
    "    return train_s1_imgs, train_s1_emds, test_s1_imgs, test_s1_emds, train_s2_imgs, train_s2_emds, test_s2_imgs, test_s2_emds, train_text, test_text\n",
    "#     return train_s1_data, test_s1_data, train_s2_data, test_s2_data, train_text, test_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StackGAN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_criterion = torch.nn.BCELoss()\n",
    "\n",
    "def KL_Loss(mu, sigma):\n",
    "    kl_ele = mu.pow(2).add_(sigma.pow(2)).mul_(-1).add_(1).add_(sigma.pow(2).log_())\n",
    "    kl = torch.mean(kl_ele).mul_(-0.5)\n",
    "    return kl\n",
    "\n",
    "def loss_gen(disc_fake, mu, sigma, kl_lambda, batch_size):\n",
    "    g_fake = -loss_criterion(disc_fake, Variable(torch.FloatTensor([0]*batch_size), requires_grad=False).cuda())\n",
    "    kl_loss = KL_Loss(mu, sigma)\n",
    "    g_total = g_fake + (kl_lambda*kl_loss)\n",
    "    return g_total\n",
    "    \n",
    "def loss_disc(disc_real, disc_fake, disc_wrong, batch_size):\n",
    "    d_real = loss_criterion(disc_real, Variable(torch.FloatTensor([1]*batch_size), requires_grad=False).cuda())\n",
    "    d_fake = loss_criterion(disc_fake, Variable(torch.FloatTensor([0]*batch_size), requires_grad=False).cuda())\n",
    "    d_wrong = loss_criterion(disc_wrong, Variable(torch.FloatTensor([0]*batch_size), requires_grad=False).cuda())\n",
    "    d_total = d_real + ((d_fake + d_wrong)*0.5)\n",
    "    return d_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upSample(c_in, c_out):\n",
    "    mod = torch.nn.Sequential(\n",
    "        torch.nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "        torch.nn.Conv2d(c_in, c_out, kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
    "        torch.nn.BatchNorm2d(c_out),\n",
    "        torch.nn.ReLU(inplace=True))\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cond_aug(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim=1024, cond_dim=128):\n",
    "        super(cond_aug,self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.cond_dim = cond_dim\n",
    "        \n",
    "        self.fc_mu = torch.nn.Linear(self.embedding_dim, self.cond_dim)\n",
    "        self.fc_sigma = torch.nn.Linear(self.embedding_dim, self.cond_dim)\n",
    "        \n",
    "        torch.nn.init.xavier_normal_(self.fc_mu.weight)\n",
    "        torch.nn.init.xavier_normal_(self.fc_sigma.weight)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x, batch_size=64):\n",
    "        mu = self.relu(self.fc_mu(x))\n",
    "        logvar = self.relu(self.fc_sigma(x))\n",
    "        sigma = logvar.mul(0.5).exp_()\n",
    "        dist = MultivariateNormal(torch.zeros(self.cond_dim).cuda(), torch.eye(self.cond_dim).cuda())\n",
    "        eps = Variable(dist.sample()).cuda().view(1,-1).repeat(batch_size,1)\n",
    "        \n",
    "        c = mu + (sigma * eps)\n",
    "        return mu, sigma, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(torch.nn.Module):\n",
    "    def __init__(self, channel_num):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.block = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(channel_num, channel_num, kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
    "            torch.nn.BatchNorm2d(channel_num),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Conv2d(channel_num, channel_num, kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
    "            torch.nn.BatchNorm2d(channel_num))\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.block(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stage1_gen(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim=1024, cond_dim=128, noise_dim=100, ups_input_dim=1024):\n",
    "        super(stage1_gen,self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.cond_dim = cond_dim\n",
    "        self.noise_dim = noise_dim\n",
    "        self.ups_input_dim = ups_input_dim\n",
    "        self.conc_dim = self.cond_dim + self.noise_dim\n",
    "        \n",
    "        self.dist = MultivariateNormal(torch.zeros(self.noise_dim).cuda(), torch.eye(self.noise_dim).cuda())\n",
    "        self.augm = cond_aug(self.embedding_dim, self.cond_dim)\n",
    "        self.ups_input = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(self.conc_dim, self.ups_input_dim*4*4, bias=False),\n",
    "                    torch.nn.BatchNorm1d(self.ups_input_dim*4*4),\n",
    "                    torch.nn.ReLU(inplace=True))\n",
    "        self.upsample1 = upSample(self.ups_input_dim,self.ups_input_dim//2)     \n",
    "        self.upsample2 = upSample(self.ups_input_dim//2,self.ups_input_dim//4)\n",
    "        self.upsample3 = upSample(self.ups_input_dim//4,self.ups_input_dim//8)\n",
    "        self.upsample4 = upSample(self.ups_input_dim//8,self.ups_input_dim//16)\n",
    "        self.gen_img = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(self.ups_input_dim//16, 3, kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
    "                torch.nn.Tanh())\n",
    "        \n",
    "    def forward(self, x, batch_size=64):\n",
    "        z = Variable(self.dist.sample()).cuda().view(1,-1).repeat(batch_size,1)\n",
    "        mu, sigma, c = self.augm(x, batch_size=batch_size)\n",
    "#         c = c.view(1,-1)\n",
    "        inp = torch.cat((c,z),1)\n",
    "        \n",
    "        x = self.ups_input(inp)\n",
    "        x = x.view(-1,self.ups_input_dim,4,4)\n",
    "        x = self.upsample1(x)\n",
    "        x = self.upsample2(x)\n",
    "        x = self.upsample3(x)\n",
    "        x = self.upsample4(x)\n",
    "        fake_img = self.gen_img(x)\n",
    "        \n",
    "        return fake_img, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stage1_disc(torch.nn.Module):\n",
    "    def __init__(self, cond_dim=128, down_dim=64):\n",
    "        super(stage1_disc,self).__init__()\n",
    "        self.cond_dim = cond_dim\n",
    "        self.down_dim = down_dim\n",
    "        \n",
    "        self.enc_img = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, self.down_dim, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            torch.nn.Conv2d(self.down_dim, self.down_dim*2, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "            torch.nn.BatchNorm2d(self.down_dim*2),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            torch.nn.Conv2d(self.down_dim*2, self.down_dim*4, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "            torch.nn.BatchNorm2d(self.down_dim*4),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            torch.nn.Conv2d(self.down_dim*4, self.down_dim*8, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "            torch.nn.BatchNorm2d(self.down_dim*8),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True))\n",
    "        \n",
    "        self.get_logits = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(self.down_dim*8+self.cond_dim, self.down_dim*8, kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
    "            torch.nn.BatchNorm2d(self.down_dim*8),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            torch.nn.Conv2d(self.down_dim*8, 1, kernel_size = 4, stride = 4),\n",
    "            torch.nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, image, cond_vec):\n",
    "        x = self.enc_img(image)\n",
    "        y = cond_vec.view(-1, self.cond_dim, 1, 1)\n",
    "        y = y.repeat(1,1,4,4)\n",
    "        z = torch.cat((x,y),1) # N x (128 + 512) x 4 x 4\n",
    "        out = self.get_logits(z).view(-1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stage2_gen(torch.nn.Module):\n",
    "    def __init__(self, down_dim=128, embedding_dim=1024, cond_dim=128, num_residuals=4):\n",
    "        super(stage2_gen,self).__init__()\n",
    "        self.down_dim = down_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.cond_dim = cond_dim\n",
    "        \n",
    "        self.downsampler = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, self.down_dim, kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(self.down_dim, self.down_dim * 2, kernel_size = 4, stride = 2, padding = 1, bias=False),\n",
    "            torch.nn.BatchNorm2d(self.down_dim * 2),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(self.down_dim * 2, self.down_dim * 4, kernel_size = 4, stride = 2, padding = 1, bias=False),\n",
    "            torch.nn.BatchNorm2d(self.down_dim * 4),\n",
    "            torch.nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.augm = cond_aug(self.embedding_dim, self.cond_dim)\n",
    "        \n",
    "        self.joint_proc = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(self.down_dim * 4 + self.cond_dim, self.down_dim * 4, kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
    "            torch.nn.BatchNorm2d(self.down_dim * 4),\n",
    "            torch.nn.ReLU(True))\n",
    "        \n",
    "        self.layers = []\n",
    "        for i in range(num_residuals):\n",
    "            self.layers.append(ResBlock(self.down_dim * 4))\n",
    "        self.residual = torch.nn.Sequential(*self.layers)\n",
    "        \n",
    "        self.upsample1 = upSample(self.down_dim * 4, self.down_dim * 2)\n",
    "        self.upsample2 = upSample(self.down_dim * 2, self.down_dim)\n",
    "        self.upsample3 = upSample(self.down_dim, self.down_dim // 2)\n",
    "        self.upsample4 = upSample(self.down_dim // 2, self.down_dim // 4)\n",
    "        self.gen_img = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(self.down_dim // 4, 3, kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
    "            torch.nn.Tanh())\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, text_embedding, stage1_image, batch_size=64):\n",
    "        encoded_img = self.downsampler(stage1_image) # --> N x 512 x 16 x 16\n",
    "        mu, sigma, c = self.augm(text_embedding, batch_size=batch_size)\n",
    "        c = c.view(-1, self.cond_dim, 1, 1) # --> N x 128 x 1 x 1\n",
    "        c = c.repeat(1, 1, 16, 16) # --> N x 128 x 16 x 16\n",
    "        conc_inp = torch.cat([encoded_img, c], 1) # --> N x 640 x 16 x 16\n",
    "        \n",
    "        conc_out = self.joint_proc(conc_inp) # --> N x 512 x 16 x 16\n",
    "        conc_out = self.residual(conc_out)   # --> N x 512 x 16 x 16\n",
    "        \n",
    "        conc_out = self.upsample1(conc_out) # --> N x 256 x 32 x 32\n",
    "        conc_out = self.upsample2(conc_out) # --> N x 128 x 64 x 64\n",
    "        conc_out = self.upsample3(conc_out) # --> N x 64 x 128 x 128\n",
    "        conc_out = self.upsample4(conc_out) # --> N x 32 x 256 x 256\n",
    "\n",
    "        fake_img = self.gen_img(conc_out) # --> N x 3 x 256 x 256\n",
    "        return fake_img, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stage2_disc(torch.nn.Module):\n",
    "    def __init__(self, cond_dim=128, down_dim=64):\n",
    "        super(stage2_disc,self).__init__()\n",
    "        self.cond_dim = cond_dim\n",
    "        self.down_dim = down_dim\n",
    "        \n",
    "        self.enc_img = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, self.down_dim, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            torch.nn.Conv2d(self.down_dim, self.down_dim*2, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "            torch.nn.BatchNorm2d(self.down_dim*2),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            torch.nn.Conv2d(self.down_dim*2, self.down_dim*4, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "            torch.nn.BatchNorm2d(self.down_dim*4),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            torch.nn.Conv2d(self.down_dim*4, self.down_dim*8, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "            torch.nn.BatchNorm2d(self.down_dim*8),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            torch.nn.Conv2d(self.down_dim*8, self.down_dim*16, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "            torch.nn.BatchNorm2d(self.down_dim*16),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            torch.nn.Conv2d(self.down_dim*16, self.down_dim*32, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "            torch.nn.BatchNorm2d(self.down_dim*32),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            torch.nn.Conv2d(self.down_dim*32, self.down_dim*16, kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
    "            torch.nn.BatchNorm2d(self.down_dim * 16),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            torch.nn.Conv2d(self.down_dim*16, self.down_dim*8, kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
    "            torch.nn.BatchNorm2d(self.down_dim * 8),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True))   # 4 * 4 * ndf * 8)\n",
    "        \n",
    "        self.get_logits = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(self.down_dim*8+self.cond_dim, self.down_dim*8, kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
    "            torch.nn.BatchNorm2d(self.down_dim*8),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            torch.nn.Conv2d(self.down_dim*8, 1, kernel_size = 4, stride = 4),\n",
    "            torch.nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, image, cond_vec):\n",
    "        x = self.enc_img(image)\n",
    "        y = cond_vec.view(-1, self.cond_dim, 1, 1)\n",
    "        y = y.repeat(1,1,4,4)\n",
    "#         print(x.shape)\n",
    "#         print(y.shape)\n",
    "        z = torch.cat((x,y),1) # N x (128 + 512) x 4 x 4\n",
    "        out = self.get_logits(z).view(-1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_s1(gen, disc, train_s1_imgs, train_s1_emds, batch_size=64, epochs=10, eta=0.0001, opt=optim.Adam,\n",
    "             model_name='A_Model_Has_No_Name', kl_lambda=2.0, embedding_dim=1024, cond_dim=128):\n",
    "    gen.cuda()\n",
    "    disc.cuda()\n",
    "    \n",
    "    optim_gen = opt(gen.parameters(), lr=eta)\n",
    "    optim_disc = opt(disc.parameters(), lr=eta)\n",
    "    \n",
    "    print('Training Stage 1...')\n",
    "    \n",
    "    l_tr_g = []\n",
    "    l_tr_d = []\n",
    "    iter_tr = []\n",
    "    \n",
    "#     n_train = len(train_s1_imgs)\n",
    "    desc_shape = train_s1_emds[0].shape\n",
    "    img_shape = train_s1_imgs[0].shape\n",
    "    \n",
    "    file_path = \"./observations/\"\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    file_path = \"./saved_models/\"\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    ts = strftime(\"%Y-%m-%d__%Hh%Mm%Ss_S1_\" + model_name, gmtime())\n",
    "    \n",
    "    real_pairs = []\n",
    "    for i in range(len(train_s1_imgs)):\n",
    "        for j in range(n_desc_per_img):\n",
    "            real_pairs.append((i,i * n_desc_per_img + j))\n",
    "    \n",
    "    n_train = len(real_pairs)\n",
    "    print('Training Size = ',n_train)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        l_tr_g_temp = 0\n",
    "        l_tr_d_temp = 0\n",
    "        \n",
    "        ############ Shuffle and Gen Fake Pairs ############\n",
    "        shuffle(real_pairs)\n",
    "        fake_pairs = []\n",
    "        for i in range(n_train):\n",
    "            nidx = randint(0,len(train_s1_emds)-1)\n",
    "            while nidx == real_pairs[i][1]:\n",
    "                nidx = randint(0,len(train_s1_emds)-1)\n",
    "            fake_pairs.append((real_pairs[i][0],nidx))\n",
    "        \n",
    "        ############ Training #############\n",
    "        start = 0\n",
    "        end = 0\n",
    "        while start < n_train:\n",
    "            end = start + batch_size\n",
    "            if end > n_train:\n",
    "                end = n_train\n",
    "            batch_size_ = end - start\n",
    "            \n",
    "            inp_img = [train_s1_imgs[x[0]] for x in real_pairs[start:end]]\n",
    "            inp_text = [train_s1_emds[x[1]] for x in real_pairs[start:end]]\n",
    "            inp_ftext = [train_s1_emds[x[1]] for x in fake_pairs[start:end]]\n",
    "            \n",
    "            real_img = Variable(torch.FloatTensor(inp_img).view(-1,img_shape[0],img_shape[1],img_shape[2])).type(torch.FloatTensor).cuda()\n",
    "            real_text = Variable(torch.FloatTensor(inp_text).view(-1,desc_shape[0])).cuda()\n",
    "            fake_text = Variable(torch.FloatTensor(inp_ftext).view(-1,desc_shape[0])).cuda()\n",
    "            \n",
    "            fake_img, mu, sigma = gen(real_text, batch_size=batch_size_)\n",
    "            \n",
    "            disc_real = disc(real_img, mu)\n",
    "            disc_fake = disc(fake_img, mu)\n",
    "            augm = cond_aug(embedding_dim, cond_dim)\n",
    "            augm.cuda()\n",
    "            mu_w, _, _ = augm(fake_text, batch_size=batch_size_)\n",
    "            disc_wrong = disc(real_img, mu_w)\n",
    "            \n",
    "            optim_gen.zero_grad()\n",
    "            optim_disc.zero_grad()\n",
    "            \n",
    "            ld_ = loss_disc(disc_real, disc_fake, disc_wrong, batch_size_)         \n",
    "            ld_.backward(retain_graph=True)\n",
    "            optim_disc.step()\n",
    "            lg_ = loss_gen(disc_fake, mu, sigma, kl_lambda, batch_size_)\n",
    "            lg_.backward(retain_graph=True)\n",
    "            optim_gen.step()\n",
    "            \n",
    "            l_tr_g_temp += float(lg_.data)\n",
    "            l_tr_d_temp += float(ld_.data)\n",
    "            \n",
    "            del inp_img, inp_text, inp_ftext, real_img, real_text, fake_text, disc_real, disc_fake, augm, mu_w, disc_wrong, ld_, lg_, batch_size_\n",
    "            \n",
    "            start += batch_size\n",
    "            \n",
    "        l_tr_g.append(l_tr_g_temp/n_train)\n",
    "        l_tr_d.append(l_tr_d_temp/n_train)\n",
    "        iter_tr.append(epoch+1)\n",
    "        l = open('./observations/Loss_S1_'+ts+'.txt','a+')\n",
    "        l.write('Epoch ' + str(epoch) + ': Generator Loss = ' + str(l_tr_g_temp/n_train) + \\\n",
    "               '         Discriminator Loss = ' + str(l_tr_d_temp/n_train)  + '\\n')\n",
    "        print('Epoch ' + str(epoch) + ': Generator Loss = ' + str(l_tr_g_temp/n_train) + \\\n",
    "               '         Discriminator Loss = ' + str(l_tr_d_temp/n_train))\n",
    "        l.close()\n",
    "    \n",
    "    return l_tr_g, l_tr_d\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_s2(gen, disc, gen1, train_s2_imgs, train_s2_emds, batch_size=64, epochs=10, eta=0.0001, opt=optim.Adam,\n",
    "             model_name='A_Model_Has_No_Name', kl_lambda=2.0, embedding_dim=1024, cond_dim=128):\n",
    "    gen.cuda()\n",
    "    disc.cuda()\n",
    "    gen1.cuda()\n",
    "    \n",
    "    optim_gen = opt(gen.parameters(), lr=eta)\n",
    "    optim_disc = opt(disc.parameters(), lr=eta)\n",
    "    \n",
    "    print('Training Stage 1...')\n",
    "    \n",
    "    l_tr_g = []\n",
    "    l_tr_d = []\n",
    "    iter_tr = []\n",
    "    \n",
    "#     n_train = len(train_s1_imgs)\n",
    "    desc_shape = train_s2_emds[0].shape\n",
    "    img_shape = train_s2_imgs[0].shape\n",
    "    \n",
    "    file_path = \"./observations/\"\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    file_path = \"./saved_models/\"\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    ts = strftime(\"%Y-%m-%d__%Hh%Mm%Ss_S1_\" + model_name, gmtime())\n",
    "    \n",
    "    real_pairs = []\n",
    "    for i in range(len(train_s2_imgs)):\n",
    "        for j in range(n_desc_per_img):\n",
    "            real_pairs.append((i,i * n_desc_per_img + j))\n",
    "    \n",
    "    n_train = len(real_pairs)\n",
    "    print('Training Size = ',n_train)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        l_tr_g_temp = 0\n",
    "        l_tr_d_temp = 0\n",
    "        \n",
    "        ############ Shuffle and Gen Fake Pairs ############\n",
    "        shuffle(real_pairs)\n",
    "        fake_pairs = []\n",
    "        for i in range(n_train):\n",
    "            nidx = randint(0,len(train_s2_emds)-1)\n",
    "            while nidx == real_pairs[i][1]:\n",
    "                nidx = randint(0,len(train_s2_emds)-1)\n",
    "            fake_pairs.append((real_pairs[i][0],nidx))\n",
    "        \n",
    "        ############ Training #############\n",
    "        start = 0\n",
    "        end = 0\n",
    "        while start < n_train:\n",
    "            end = start + batch_size\n",
    "            if end > n_train:\n",
    "                end = n_train\n",
    "            batch_size_ = end - start\n",
    "            \n",
    "            inp_img = [train_s2_imgs[x[0]] for x in real_pairs[start:end]]\n",
    "            inp_text = [train_s2_emds[x[1]] for x in real_pairs[start:end]]\n",
    "            inp_ftext = [train_s2_emds[x[1]] for x in fake_pairs[start:end]]\n",
    "            \n",
    "            real_img = Variable(torch.FloatTensor(inp_img).view(-1,img_shape[0],img_shape[1],img_shape[2])).type(torch.FloatTensor).cuda()\n",
    "            real_text = Variable(torch.FloatTensor(inp_text).view(-1,desc_shape[0])).cuda()\n",
    "            fake_text = Variable(torch.FloatTensor(inp_ftext).view(-1,desc_shape[0])).cuda()\n",
    "            \n",
    "            fake_s1 = gen1(real_text, batch_size=batch_size_)[0]\n",
    "            fake_img, mu, sigma = gen(real_text, fake_s1, batch_size=batch_size_)\n",
    "            \n",
    "            disc_real = disc(real_img, mu)\n",
    "            disc_fake = disc(fake_img, mu)\n",
    "            augm = cond_aug(embedding_dim, cond_dim)\n",
    "            augm.cuda()\n",
    "            mu_w, _, _ = augm(fake_text, batch_size=batch_size_)\n",
    "            disc_wrong = disc(real_img, mu_w)\n",
    "            \n",
    "            optim_gen.zero_grad()\n",
    "            optim_disc.zero_grad()\n",
    "            \n",
    "            ld_ = loss_disc(disc_real, disc_fake, disc_wrong, batch_size_)         \n",
    "            ld_.backward(retain_graph=True)\n",
    "            optim_disc.step()\n",
    "            lg_ = loss_gen(disc_fake, mu, sigma, kl_lambda, batch_size_)\n",
    "            lg_.backward(retain_graph=True)\n",
    "            optim_gen.step()\n",
    "            \n",
    "            l_tr_g_temp += float(lg_.data)\n",
    "            l_tr_d_temp += float(ld_.data)\n",
    "            \n",
    "            del inp_img, inp_text, inp_ftext, real_img, real_text, fake_text, disc_real, disc_fake, augm, mu_w, disc_wrong, ld_, lg_, batch_size_, fake_s1\n",
    "            \n",
    "            print(start)\n",
    "            start += batch_size\n",
    "            \n",
    "        l_tr_g.append(l_tr_g_temp/n_train)\n",
    "        l_tr_d.append(l_tr_d_temp/n_train)\n",
    "        iter_tr.append(epoch+1)\n",
    "        l = open('./observations/Loss_S1_'+ts+'.txt','a+')\n",
    "        l.write('Epoch ' + str(epoch) + ': Generator Loss = ' + str(l_tr_g_temp/n_train) + \\\n",
    "               '         Discriminator Loss = ' + str(l_tr_d_temp/n_train)  + '\\n')\n",
    "        print('Epoch ' + str(epoch) + ': Generator Loss = ' + str(l_tr_g_temp/n_train) + \\\n",
    "               '         Discriminator Loss = ' + str(l_tr_d_temp/n_train))\n",
    "        l.close()\n",
    "    \n",
    "    return l_tr_g, l_tr_d\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_s1(gen, disc, test_s1_data, test_text, model_name='A_Model_Has_No_Name'):\n",
    "    print('Testing Stage 1...')\n",
    "    \n",
    "    n_test = len(test_s1_data)\n",
    "    desc_shape = test_s1_data[0][1].shape\n",
    "    \n",
    "    ts = strftime(\"%Y-%m-%d__%Hh%Mm%Ss_S1_\" + model_name, gmtime())\n",
    "    file_path = \"./gen_imgs/\"\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    for i in range(n_test):\n",
    "        text = Variable(torch.from_numpy(test_s1_data[i][1]).view(-1,desc_shape[0])).cuda()\n",
    "        fake_img = gen(text)[0]\n",
    "        out_shape = fake_img.shape\n",
    "        fake_img_ = np.transpose(fake_img.cpu().view(out_shape[1],out_shape[2],out_shape[3]).data.numpy(),(1,2,0))\n",
    "        idx = i // 2\n",
    "        im = Image.fromarray(fake_img_.astype('uint8'))\n",
    "        save_text = test_text[idx]\n",
    "        if save_text[-1] == '.':\n",
    "            save_text = save_text[:-1]\n",
    "        im.save(file_path+ts+save_text+'.jpg')\n",
    "        i += 1\n",
    "        \n",
    "        del text, fake_img, fake_img_, im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_s2(gen, disc, gen1, test_s2_data, test_text, model_name='A_Model_Has_No_Name'):\n",
    "    print('Testing Stage 2...')\n",
    "    \n",
    "    n_test = len(test_s2_data)\n",
    "    desc_shape = test_s2_data[0][1].shape\n",
    "    \n",
    "    ts = strftime(\"%Y-%m-%d__%Hh%Mm%Ss_S2_\" + model_name, gmtime())\n",
    "    file_path = \"./gen_imgs/\"\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    for i in range(n_test):\n",
    "        text = Variable(torch.from_numpy(test_s2_data[i][1]).view(-1,desc_shape[0])).cuda()\n",
    "        fake_s1 = gen1(text)[0]\n",
    "        fake_img = gen(text, fake_s1)[0]\n",
    "        out_shape = fake_img.shape\n",
    "        fake_img_ = np.transpose(fake_img.cpu().view(out_shape[1],out_shape[2],out_shape[3]).data.numpy(),(1,2,0))\n",
    "        idx = i // 2\n",
    "        im = Image.fromarray(fake_img_.astype('uint8'))\n",
    "        save_text = test_text[idx]\n",
    "        if save_text[-1] == '.':\n",
    "            save_text = save_text[:-1]\n",
    "        im.save(file_path+ts+save_text+'.jpg')\n",
    "        i += 1\n",
    "        \n",
    "        del text, fake_s1, fake_img, fake_img_, im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def driver(train_s1_data, test_s1_data, train_s2_data, test_s2_data, train_text, test_text, batch_size=64, \n",
    "           epochs_s1=10, epochs_s2=10, eta_s1=0.0001, eta_s2=0.0001, opt=optim.Adam, embedding_dim=1024, \n",
    "           cond_dim=128, noise_dim=100, ups_input_dim=1024, down_dim=128, num_residuals=4, \n",
    "           kl_lambda=2.0, model_name = 'A_Model_Has_No_Name'):\n",
    "    print('######### Processing Model ',model_name,' ###############')\n",
    "    \n",
    "    print('Models Initializaed ==>')\n",
    "    gen_s1 = stage1_gen(embedding_dim=embedding_dim, cond_dim=cond_dim, noise_dim=noise_dim, ups_input_dim=ups_input_dim, batch_size=batch_size)\n",
    "    disc_s1 = stage1_disc(cond_dim=cond_dim, down_dim=down_dim, batch_size=batch_size)\n",
    "    gen_s2 = stage2_gen(down_dim=down_dim, embedding_dim=embedding_dim, cond_dim=cond_dim, num_residuals=num_residuals, batch_size=batch_size)\n",
    "    disc_s2 = stage2_disc(cond_dim=cond_dim, down_dim=down_dim, batch_size=batch_size)\n",
    "    \n",
    "    print('Training Begins ==>')\n",
    "    l_tr_g_s1, l_tr_d_s1 = train_s1(gen_s1, disc_s1, train_s1_data, batch_size=batch_size, epochs=epochs_s1,\n",
    "            eta=eta_s1, opt=optim.Adam, model_name=model_name, kl_lambda=kl_lambda,\n",
    "            embedding_dim=embedding_dim, cond_dim=cond_dim)\n",
    "    l_tr_g_s2, l_tr_d_s2 = train_s2(gen_s2, disc_s2, gen_s1, train_s2_data, epochs=epochs_s2, eta=eta_s2, opt=optim.Adam, model_name=model_name,\n",
    "             kl_lambda=kl_lambda, embedding_dim=embedding_dim, cond_dim=cond_dim)\n",
    "    \n",
    "    print('Testing Begins ==>')\n",
    "    test_s1(gen_s1, disc_s1, test_s1_data, test_text, model_name=model_name)\n",
    "    test_s2(gen_s2, disc_s2, gen_s1, test_s2_data, test_text, model_name=model_name)\n",
    "    \n",
    "    print('Done! Check Out the results...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data Input ==>')\n",
    "train_s1_imgs, train_s1_emds, test_s1_imgs, test_s1_emds, train_s2_imgs, train_s2_emds, test_s2_imgs, test_s2_emds, train_text, test_text = read_input()\n",
    "print('Train_S1_Data : ',len(train_s1_imgs))\n",
    "print('Test_S1_Data : ',len(test_s1_imgs))\n",
    "print('Train_S2_Data : ',len(train_s2_imgs))\n",
    "print('Test_S2_Data : ',len(test_s2_imgs))\n",
    "print('Train_S1_Data : ',len(train_s1_emds))\n",
    "print('Test_S1_Data : ',len(test_s1_emds))\n",
    "print('Train_S2_Data : ',len(train_s2_emds))\n",
    "print('Test_S2_Data : ',len(test_s2_emds))\n",
    "print('Train_Text : ',len(train_text))\n",
    "print('Test_Text : ',len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs_s1 = 600\n",
    "epochs_s2 = 600\n",
    "eta_s1 = 0.0001\n",
    "eta_s2 = 0.0001\n",
    "opt = optim.Adam\n",
    "embedding_dim = 1024\n",
    "cond_dim = 128\n",
    "noise_dim = 100\n",
    "ups_input_dim = 1024\n",
    "down_dim = 128\n",
    "num_residuals = 4\n",
    "kl_lambda = 2.0\n",
    "model_name = 'Trial_1'\n",
    "\n",
    "# driver(train_s1_data, test_s1_data, train_s2_data, test_s2_data, train_text, test_text, epochs_s1=epochs_s1, \n",
    "#        epochs_s2=epochs_s2, eta_s1=eta_s1, eta_s2=eta_s2, opt=opt, embedding_dim=embedding_dim, cond_dim=cond_dim,\n",
    "#        noise_dim=noise_dim, ups_input_dim=ups_input_dim, down_dim=down_dim, num_residuals=num_residuals,\n",
    "#         kl_lambda=kl_lambda, model_name = model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_s1 = stage1_gen(embedding_dim=embedding_dim, cond_dim=cond_dim, noise_dim=noise_dim, ups_input_dim=ups_input_dim)\n",
    "disc_s1 = stage1_disc(cond_dim=cond_dim, down_dim=down_dim)\n",
    "gen_s2 = stage2_gen(down_dim=down_dim, embedding_dim=embedding_dim, cond_dim=cond_dim, num_residuals=num_residuals)\n",
    "disc_s2 = stage2_disc(cond_dim=cond_dim, down_dim=down_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "# l_tr_g_s1, l_tr_d_s1 = train_s1(gen_s1, disc_s1, train_s1_imgs, train_s1_emds, batch_size=batch_size,\n",
    "#             epochs=epochs_s1, eta=eta_s1, opt=opt, model_name=model_name, kl_lambda=kl_lambda,\n",
    "#             embedding_dim=embedding_dim, cond_dim=cond_dim)\n",
    "l_tr_g_s2, l_tr_d_s2 = train_s2(gen_s2, disc_s2, gen_s1, train_s2_imgs, train_s2_emds, batch_size=batch_size,\n",
    "            epochs=epochs_s2, eta=eta_s2, opt=opt, model_name=model_name, kl_lambda=kl_lambda,\n",
    "            embedding_dim=embedding_dim, cond_dim=cond_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,4,5,6,7,8,9]\n",
    "shuffle(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
